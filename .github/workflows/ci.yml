name: Rust CI

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - '*'

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1
  RUSTFLAGS: "-C debuginfo=line-tables-only -C incremental=false"

jobs:
  check:
    name: Basic check
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: dtolnay/rust-toolchain@master
        with:
          toolchain: nightly-2025-04-29

      - uses: Swatinem/rust-cache@v2
      
      - name: Check formatting
        run: cargo fmt --all -- --check

      - name: Check documentation
        run: cargo doc --no-deps --document-private-items
        env:
          RUSTDOCFLAGS: -D warnings

      - name: Run clippy
        run: cargo clippy -- -D warnings

  unit_test:
    name: Unit Test
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: dtolnay/rust-toolchain@master
        with:
          toolchain: nightly-2025-04-29
      - uses: Swatinem/rust-cache@v2
      - name: Install cargo-llvm-cov
        uses: taiki-e/install-action@cargo-llvm-cov
      - name: Generate code coverage
        run: cargo llvm-cov --workspace --codecov --output-path codecov.json
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5
        with:
          token: ${{ secrets.CODECOV_TOKEN }} 
          files: codecov.json
          fail_ci_if_error: true
  
  shuttle_test:
    name: Shuttle Test
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: dtolnay/rust-toolchain@master
        with:
          toolchain: nightly-2025-04-29
      - uses: Swatinem/rust-cache@v2
      - name: Run shuttle test
        run: |
          cd src/liquid_parquet
          cargo test --features "shuttle" --release -- --test-threads=1 shuttle 

  address_san:
    name: Address Sanitizer
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: dtolnay/rust-toolchain@master
        with:
          toolchain: nightly-2025-04-29
      # Address sanitizers can't be cached: https://github.com/Swatinem/rust-cache/issues/161
      - run: sudo apt install llvm-dev -y
      - name: Run address sanitizer
        run: >
          env RUSTFLAGS="-Z sanitizer=address" cargo test -Zbuild-std --target x86_64-unknown-linux-gnu --tests -p liquid-cache-parquet

  clickbench:
    name: ClickBench
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: dtolnay/rust-toolchain@master
        with:
          toolchain: nightly-2025-04-29
      - run: sudo apt-get update && sudo apt-get install -y wget
      - name: Install cargo-llvm-cov
        uses: taiki-e/install-action@cargo-llvm-cov
      - uses: Swatinem/rust-cache@v2
      - name: Download ClickBench partition 0
        run: |
          mkdir -p benchmark/data
          wget https://datasets.clickhouse.com/hits_compatible/athena_partitioned/hits_0.parquet -O benchmark/data/hits_0.parquet
      - name: Run ClickBench
        run: |
          source <(cargo llvm-cov show-env --export-prefix)
          cargo llvm-cov clean --workspace
          cargo build --bin bench_server
          cargo build --bin clickbench_client
          env RUST_LOG=info nohup cargo run --bin bench_server -- --abort-on-panic --cache-mode liquid_eager_transcode &> server.log &
          sleep 2  # Wait for server to start up
          env RUST_LOG=info cargo run --bin clickbench_client -- --query-path benchmark/clickbench/queries/queries.sql --file examples/nano_hits.parquet
          echo "=== Server logs ==="
          cat server.log || echo "No server log found"
          curl http://localhost:53703/shutdown
          env RUST_LOG=info nohup cargo run --bin bench_server -- --abort-on-panic --cache-mode liquid_eager_transcode &> server.log &
          sleep 2  # Wait for server to start up
          env RUST_LOG=info cargo run --bin clickbench_client -- --query-path benchmark/clickbench/queries/queries.sql --file benchmark/data/hits_0.parquet --answer-dir benchmark/clickbench/answers/clickbench_0
          echo "=== Server logs (partition 0) ==="
          cat server.log || echo "No server log found"
            curl http://localhost:53703/shutdown
          cargo llvm-cov report --codecov --output-path codecov_clickbench.json
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          files: codecov_clickbench.json
          fail_ci_if_error: true

  tpch:
    name: TPC-H
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: dtolnay/rust-toolchain@master
        with:
          toolchain: nightly-2025-04-29
      - run: sudo apt-get update && sudo apt-get install -y wget
      - name: Install cargo-llvm-cov
        uses: taiki-e/install-action@cargo-llvm-cov
      - uses: Swatinem/rust-cache@v2
      - name: Setup TPC-H data
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
          cd benchmark/tpch
          uvx --from duckdb python tpch_gen.py --scale 0.1
      - name: Run TPC-H
        run: |
          source <(cargo llvm-cov show-env --export-prefix)
          cargo llvm-cov clean --workspace
          cargo build --bin bench_server
          cargo build --bin tpch_client
          env RUST_LOG=info nohup cargo run --bin bench_server -- --abort-on-panic --cache-mode liquid_eager_transcode &> server.log &
          sleep 2  # Wait for server to start up
          env RUST_LOG=info cargo run --bin tpch_client -- --query-dir benchmark/tpch/queries --answer-dir benchmark/tpch/answers/sf0.1 --data-dir benchmark/tpch/data/sf0.1
          echo "=== Server logs ==="
          cat server.log || echo "No server log found"
          curl http://localhost:53703/shutdown
          cargo llvm-cov report --codecov --output-path codecov_tpch.json
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          files: codecov_tpch.json
          fail_ci_if_error: true

  benchmark:
    name: Performance Benchmark
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    permissions:
      contents: write
      pull-requests: write
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}
      - uses: dtolnay/rust-toolchain@master
        with:
          toolchain: nightly-2025-04-29
      - run: sudo apt-get update && sudo apt-get install -y wget jq
      - uses: Swatinem/rust-cache@v2

      - name: Setup ClickBench partitioned data download
        run: |
          mkdir -p benchmark/clickbench/data
          for partition in 0 1 2 3; do
            echo "Downloading partition ${partition}..."
            wget "https://datasets.clickhouse.com/hits_compatible/athena_partitioned/hits_${partition}.parquet" \
              -O "benchmark/clickbench/data/hits_${partition}.parquet"
          done

      - name: Update manifest for partitioned data
        run: |
          # Create manifest that uses partitioned data and external queries file
          cat > benchmark/clickbench/benchmark_manifest.json << 'EOF'
          {
            "name": "ClickBench Partitioned",
            "tables": {
              "hits": "benchmark/clickbench/data"
            },
            "queries": "benchmark/clickbench/queries/queries.sql"
          }
          EOF

      - name: Build benchmark binary
        run: cargo build --release --bin in_process

      - name: Run benchmark
        run: |
          mkdir -p benchmark_results
          env RUST_LOG=info cargo run --release --bin in_process -- \
            --manifest benchmark/clickbench/benchmark_manifest.json \
            --output benchmark_results/benchmark.json \
            --iteration 3 \
            --bench-mode liquid-eager-transcode

      - name: Create timestamped benchmark result
        run: |
          jq --arg timestamp "$(date -Iminutes)" --arg commit "${{ github.sha }}" \
             '. + {"timestamp": $timestamp, "commit": $commit}' \
             benchmark_results/benchmark.json > benchmark_results/final.json

      - name: Setup benchmark branch
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          
          # Try to fetch the benchmark branch
          git fetch origin benchmark:benchmark 2>/dev/null || true
          git checkout benchmark 2>/dev/null || git checkout -b benchmark

      - name: Save benchmark results to branch
        run: |
          mkdir -p benchmarks
          cp benchmark_results/final.json "benchmarks/${{ github.sha }}.json"
          git add "benchmarks/${{ github.sha }}.json"
          git commit -m "Add benchmark results for ${{ github.sha }}" || echo "No changes to commit"
          git push origin benchmark || git push --set-upstream origin benchmark

      - name: Compare with main branch baseline
        id: compare
        run: |
          # Get the latest commit from main branch
          main_commit=$(git log origin/main --format="%H" -n 1)
          baseline_benchmark="benchmarks/${main_commit}.json"
          
          echo "Looking for baseline benchmark: $baseline_benchmark"
          
          if [ -f "$baseline_benchmark" ]; then
            echo "Comparing with main branch commit: ${main_commit:0:8}"
            
            # Use the external Python script for comparison
            python3 .github/compare_benchmarks.py \
              "benchmark_results/final.json" \
              "$baseline_benchmark" \
              --output comparison.md
            
            echo "COMPARISON_AVAILABLE=true" >> $GITHUB_OUTPUT
          else
            echo "No baseline benchmark found for main branch commit: ${main_commit:0:8}"
            echo "Expected file: $baseline_benchmark"
            
            # Create a simple message for first-time runs
            cat > comparison.md << 'EOF'
          ## ðŸ“Š Benchmark Comparison
          
          **No baseline found** - This appears to be the first benchmark run or the main branch doesn't have benchmark data yet.
          
          The benchmark results have been saved and will be used as a baseline for future comparisons.
          
          âœ… Benchmark completed successfully
          EOF
            echo "COMPARISON_AVAILABLE=true" >> $GITHUB_OUTPUT
          fi

      - name: Comment PR with benchmark results
        if: steps.compare.outputs.COMPARISON_AVAILABLE == 'true'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            let comment = '';
            try {
              comment = fs.readFileSync('comparison.md', 'utf8');
            } catch (error) {
              comment = 'Error reading benchmark comparison results';
            }
            
            // Find existing benchmark comment
            const comments = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });
            
            const botComment = comments.data.find(comment => 
              comment.user.type === 'Bot' && 
              comment.body.includes('## ðŸ“Š Benchmark Comparison')
            );
            
            if (botComment) {
              // Update existing comment
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: comment
              });
            } else {
              // Create new comment
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment
              });
            }

  examples:
    name: Run client/server examples (${{ matrix.os }})
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, macos-latest]
      fail-fast: false
    steps:
      - uses: actions/checkout@v4
      - uses: dtolnay/rust-toolchain@master
        with:
          toolchain: nightly-2025-04-29

      - uses: Swatinem/rust-cache@v2

      - name: Build LiquidCache server
        run: cargo build --bin example_server
      - name: Build LiquidCache client
        run: cargo build --bin example_client

      - name: Start LiquidCache server
        run: |
          env RUST_LOG=info nohup cargo run --bin example_server -- --abort-on-panic &> server.log &
          echo $! > server.pid  # Save PID for later cleanup
          sleep 2  # Wait for server to start up

      - name: Start LiquidCache client
        run: |
          # First run to populate the cache
          env RUST_LOG=info cargo run --bin example_client
          # Run twice to test the cache
          env RUST_LOG=info cargo run --bin example_client

      - name: Kill LiquidCache server and show logs
        if: always()
        run: |
          echo "=== Server logs ==="
          cat server.log || echo "No server log found"
          pkill -F server.pid || true
          rm -f server.pid
