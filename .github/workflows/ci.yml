name: Rust CI

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - '*'

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1
  RUSTFLAGS: "-C debuginfo=line-tables-only -C incremental=false"

jobs:
  check:
    name: Basic check
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: dtolnay/rust-toolchain@stable
      - uses: Swatinem/rust-cache@v2
      
      - name: Check formatting
        run: cargo fmt --all -- --check

      - name: Check documentation
        run: cargo doc --no-deps --document-private-items
        env:
          RUSTDOCFLAGS: -D warnings

      - name: Run clippy
        run: cargo clippy --all-targets --all-features -- -D warnings

  unit_test:
    name: Unit Test
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: dtolnay/rust-toolchain@stable
      - uses: Swatinem/rust-cache@v2
      - name: Install cargo-llvm-cov
        uses: taiki-e/install-action@cargo-llvm-cov
      - name: Generate code coverage
        run: cargo llvm-cov --workspace --codecov --output-path codecov.json
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5
        with:
          token: ${{ secrets.CODECOV_TOKEN }} 
          files: codecov.json
          fail_ci_if_error: true
  
  shuttle_test:
    name: Shuttle Test
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: dtolnay/rust-toolchain@stable
      - uses: Swatinem/rust-cache@v2
      - name: Run shuttle test
        run: |
          cd src/liquid_parquet
          cargo test --features "shuttle" --release -- --test-threads=1 shuttle 

  address_san:
    name: Address Sanitizer
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      # Sanitizers can only run on nightly
      - uses: dtolnay/rust-toolchain@nightly
        with:
          components: rust-src

      # Address sanitizers can't be cached: https://github.com/Swatinem/rust-cache/issues/161
      - run: sudo apt install llvm-dev -y
      - name: Run address sanitizer
        run: >
          env RUSTFLAGS="-Z sanitizer=address" cargo test -Zbuild-std --target x86_64-unknown-linux-gnu --tests -p liquid-cache-parquet

  clickbench:
    name: ClickBench
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: dtolnay/rust-toolchain@stable
      - run: sudo apt-get update && sudo apt-get install -y wget
      - name: Install cargo-llvm-cov
        uses: taiki-e/install-action@cargo-llvm-cov
      - uses: Swatinem/rust-cache@v2
      - name: Download ClickBench partition 0
        run: |
          mkdir -p benchmark/data
          wget https://datasets.clickhouse.com/hits_compatible/athena_partitioned/hits_0.parquet -O benchmark/data/hits_0.parquet
      - name: Update manifest for partitioned data
        run: |
          # Update the manifest to point to the partitioned data directory
          sed 's|"benchmark/clickbench/data/hits.parquet"|"benchmark/clickbench/data"|' \
            benchmark/clickbench/manifest.json > benchmark/clickbench/benchmark_manifest.json

      - name: Run ClickBench
        run: |
          source <(cargo llvm-cov show-env --export-prefix)
          cargo llvm-cov clean --workspace
          cargo build --bin bench_server
          cargo build --bin clickbench_client
          env RUST_LOG=info nohup cargo run --bin bench_server -- --abort-on-panic --cache-mode liquid_eager_transcode &> server.log &
          sleep 2  # Wait for server to start up
          env RUST_LOG=info cargo run --bin clickbench_client -- --manifest benchmark/clickbench/benchmark_manifest.json
          echo "=== Server logs ==="
          cat server.log || echo "No server log found"
          curl http://localhost:53703/shutdown
          env RUST_LOG=info nohup cargo run --bin bench_server -- --abort-on-panic --cache-mode liquid_eager_transcode &> server.log &
          sleep 2  # Wait for server to start up
          env RUST_LOG=info cargo run --bin clickbench_client -- --manifest benchmark/clickbench/benchmark_manifest.json --answer-dir benchmark/clickbench/answers/clickbench_0
          echo "=== Server logs (partition 0) ==="
          cat server.log || echo "No server log found"
            curl http://localhost:53703/shutdown
          cargo llvm-cov report --codecov --output-path codecov_clickbench.json
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          files: codecov_clickbench.json
          fail_ci_if_error: true

  tpch:
    name: TPC-H
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: dtolnay/rust-toolchain@stable
      - run: sudo apt-get update && sudo apt-get install -y wget
      - name: Install cargo-llvm-cov
        uses: taiki-e/install-action@cargo-llvm-cov
      - uses: Swatinem/rust-cache@v2
      - name: Setup TPC-H data
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
          cd benchmark/tpch
          uvx --from duckdb python tpch_gen.py --scale 0.1
      - name: Run TPC-H
        run: |
          source <(cargo llvm-cov show-env --export-prefix)
          cargo llvm-cov clean --workspace
          cargo build --bin bench_server
          cargo build --bin tpch_client
          env RUST_LOG=info nohup cargo run --bin bench_server -- --abort-on-panic --cache-mode liquid_eager_transcode &> server.log &
          sleep 2  # Wait for server to start up
          env RUST_LOG=info cargo run --bin tpch_client -- --query-dir benchmark/tpch/queries --answer-dir benchmark/tpch/answers/sf0.1 --data-dir benchmark/tpch/data/sf0.1
          echo "=== Server logs ==="
          cat server.log || echo "No server log found"
          curl http://localhost:53703/shutdown
          cargo llvm-cov report --codecov --output-path codecov_tpch.json
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          files: codecov_tpch.json
          fail_ci_if_error: true

  benchmark:
    name: Performance Benchmark
    runs-on: ubicloud-standard-4-ubuntu-2404
    permissions:
      contents: write
      pull-requests: write
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}
      - uses: dtolnay/rust-toolchain@stable
      - run: sudo apt-get update && sudo apt-get install -y wget jq
      - uses: ubicloud/rust-cache@v2

      - name: Setup ClickBench partitioned data download
        run: |
          mkdir -p benchmark/clickbench/data
          for partition in 0 1 2 3; do
            echo "Downloading partition ${partition}..."
            wget "https://datasets.clickhouse.com/hits_compatible/athena_partitioned/hits_${partition}.parquet" \
              -O "benchmark/clickbench/data/hits_${partition}.parquet"
          done

      - name: Update manifest for partitioned data
        run: |
          # Update the manifest to point to the partitioned data directory
          sed 's|"benchmark/clickbench/data/hits.parquet"|"benchmark/clickbench/data"|' \
            benchmark/clickbench/manifest.json > benchmark/clickbench/benchmark_manifest.json

      - name: Build benchmark binary
        run: cargo build --release --bin in_process

      - name: Run benchmark
        run: |
          mkdir -p benchmark_results
          env RUST_LOG=info cargo run --release --bin in_process -- \
            --manifest benchmark/clickbench/benchmark_manifest.json \
            --output benchmark_results/benchmark.json \
            --iteration 5 \
            --bench-mode liquid-eager-transcode

      - name: Create timestamped benchmark result
        run: |
          jq --arg timestamp "$(date -Iminutes)" --arg commit "${{ github.sha }}" \
             '. + {"timestamp": $timestamp, "commit": $commit}' \
             benchmark_results/benchmark.json > benchmark_results/final.json

      - name: Setup benchmark branch
        if: github.ref == 'refs/heads/main'
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          
          # Try to fetch the benchmark branch
          git fetch origin benchmark:benchmark 2>/dev/null || true
          git checkout benchmark 2>/dev/null || git checkout -b benchmark

      - name: Save benchmark results to branch
        if: github.ref == 'refs/heads/main'
        run: |
          mkdir -p benchmarks
          cp benchmark_results/final.json "benchmarks/${{ github.sha }}.json"
          git add "benchmarks/${{ github.sha }}.json"
          git commit -m "Add benchmark results for ${{ github.sha }}" || echo "No changes to commit"
          git push origin benchmark || git push --set-upstream origin benchmark

      - name: Compare with main branch baseline
        id: compare
        run: |
          # Fetch the benchmark branch to access existing benchmarks for comparison
          git fetch origin benchmark:benchmark 2>/dev/null || true
          
          # Save the comparison script before switching branches
          cp .github/compare_benchmarks.py /tmp/compare_benchmarks.py
          
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            # For PRs, compare against the latest main branch commit benchmark
            main_commit=$(git log origin/main --format="%H" -n 1)
            baseline_benchmark="benchmarks/${main_commit}.json"
            
            # Switch to benchmark branch to check for baseline
            git checkout benchmark 2>/dev/null || echo "Benchmark branch not found"
            
            if [ -f "$baseline_benchmark" ]; then
              echo "Comparing with main branch commit: ${main_commit:0:8}"
              
              # Use the external Python script for comparison (from saved copy)
              python3 /tmp/compare_benchmarks.py \
                "benchmark_results/final.json" \
                "$baseline_benchmark" \
                --output comparison.md
              
              echo "COMPARISON_AVAILABLE=true" >> $GITHUB_OUTPUT
            else
              echo "No baseline benchmark found for main branch commit: ${main_commit:0:8}"
              
              # Find the most recent benchmark as fallback
              if ls benchmarks/*.json >/dev/null 2>&1; then
                baseline_benchmark=$(ls -t benchmarks/*.json | head -n 1)
                baseline_commit=$(basename "$baseline_benchmark" .json)
                
                echo "Using most recent benchmark as baseline: ${baseline_commit:0:8}"
                
                # Use the external Python script for comparison (from saved copy)
                python3 /tmp/compare_benchmarks.py \
                  "benchmark_results/final.json" \
                  "$baseline_benchmark" \
                  --output comparison.md
                
                echo "COMPARISON_AVAILABLE=true" >> $GITHUB_OUTPUT
              else
                cat > comparison.md << 'EOF'
          ## ðŸ“Š Benchmark Comparison
          
          **No baseline found** - This appears to be the first benchmark run.
          
          The benchmark results have been saved and will be used as a baseline for future comparisons.
          
          âœ… Benchmark completed successfully
          EOF
                echo "COMPARISON_AVAILABLE=true" >> $GITHUB_OUTPUT
              fi
            fi
          else
            # For main branch runs, just indicate successful completion
            cat > comparison.md << 'EOF'
          ## ðŸ“Š Benchmark Completed
          
          âœ… Benchmark completed successfully and results saved to benchmark branch.
          
          This baseline will be used for future PR comparisons.
          EOF
            echo "COMPARISON_AVAILABLE=true" >> $GITHUB_OUTPUT
          fi

      - name: Comment PR with benchmark results
        if: steps.compare.outputs.COMPARISON_AVAILABLE == 'true' && github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            let comment = '';
            try {
              comment = fs.readFileSync('comparison.md', 'utf8');
            } catch (error) {
              comment = 'Error reading benchmark comparison results';
            }
            
            // Check if this is an external PR (from a fork)
            const isExternalPR = context.payload.pull_request.head.repo.full_name !== context.payload.pull_request.base.repo.full_name;
            
            if (isExternalPR) {
              console.log('Skipping comment for external PR due to permission restrictions');
              console.log('Benchmark results:');
              console.log(comment);
              return;
            }
            
            try {
              // Find existing benchmark comment
              const comments = await github.rest.issues.listComments({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
              });
              
              const botComment = comments.data.find(comment => 
                comment.user.type === 'Bot' && 
                comment.body.includes('## ðŸ“Š Benchmark Comparison')
              );
              
              if (botComment) {
                // Update existing comment
                await github.rest.issues.updateComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  comment_id: botComment.id,
                  body: comment
                });
              } else {
                // Create new comment
                await github.rest.issues.createComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  issue_number: context.issue.number,
                  body: comment
                });
              }
            } catch (error) {
              console.log('Failed to post comment, likely due to permissions:', error.message);
              console.log('Benchmark results:');
              console.log(comment);
            }

  examples:
    name: Run client/server examples (${{ matrix.os }})
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, macos-latest]
      fail-fast: false
    steps:
      - uses: actions/checkout@v4
      - uses: dtolnay/rust-toolchain@stable
      - uses: Swatinem/rust-cache@v2

      - name: Build LiquidCache server
        run: cargo build --bin example_server
      - name: Build LiquidCache client
        run: cargo build --bin example_client

      - name: Start LiquidCache server
        run: |
          env RUST_LOG=info nohup cargo run --bin example_server -- --abort-on-panic &> server.log &
          echo $! > server.pid  # Save PID for later cleanup
          sleep 2  # Wait for server to start up

      - name: Start LiquidCache client
        run: |
          # First run to populate the cache
          env RUST_LOG=info cargo run --bin example_client
          # Run twice to test the cache
          env RUST_LOG=info cargo run --bin example_client

      - name: Kill LiquidCache server and show logs
        if: always()
        run: |
          echo "=== Server logs ==="
          cat server.log || echo "No server log found"
          pkill -F server.pid || true
          rm -f server.pid
